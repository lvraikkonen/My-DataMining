install.packages('rJava')
library(tm)
install.packages("Rwordseg", repos="http://R-Forge.R-project.org", type="source")
library("rJava", lib.loc="C:/Users/v-shuolv/Documents/R/win-library/3.1")
Sys.getenv("JAVA_HOME")
library(rJava)
library(rJava)
library(tm)
library(Rwordseg)
library(RTextTools)
library(FSelector)
install.packages("Rwordseg", repos="http://R-Forge.R-project.org", type="source")
install.packages("Rwordseg", repos="http://R-Forge.R-project.org")
install.packages("~/Rwordseg_0.2-1.zip", repos = NULL)
library(tm)
library(Rwordseg)
library(RTextTools)
library(FSelector)
teststring1 <- "我是一个俗人"
teststring1
segmentCN(teststring1)
segmentCN(c("`(¢3n", "g5õa"))
teststring1 <- "我沉默不代表我不痛我不痛眼泪就不会流总是安静承受安静忍受安静看你走你说我很适合当朋友"
segmentCN(teststring1)
library(Rwordseg)
teststring <- "采用两种粒度的搜索粗粒度和细粒度"
segmentCN(teststring)
set.seed(1)
rpois(5, 2)
a <- rpois(5, 2)
mode(a)
library(datasets)
Rprof()
fit <- lm(y ~ x1 + x2)
Rprof(NULL)
library(datasets)
Rprof()
fit <- lm(y ~ x1 + x2)
summaryRprof()
y <- rnorm(10)
download.file('https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv')
?download.file
url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv'
destFile <- "./data.csv"
download.file(url,destFile,method="curl")
library(httr)
library(httpuv)
library(jsonlite)
?oauth_app
oauth_endpoints("twitter")
# 2. Register an application at https://github.com/settings/applications
# Insert your values below - if secret is omitted, it will look it up in the
# GITHUB_CONSUMER_SECRET environmental variable.  Use http://localhost:1410
# as the callback url
myapp <- oauth_app("ContentMining", "an3hwBQXNU7xwZmz3ISTHHryI", secret = " I5pMQT5Hlh7G3zHhIBmmTC2f0AKCrJKBem69eTygwpfRu9hNBj")
# 3. Get OAuth credentials
twitter_token <- oauth2.0_token(oauth_endpoints("twitter"), myapp)
oauth_endpoints("twitter")
myapp <- oauth_app("ContentMining", "an3hwBQXNU7xwZmz3ISTHHryI", secret = " I5pMQT5Hlh7G3zHhIBmmTC2f0AKCrJKBem69eTygwpfRu9hNBj")
twitter_token <- oauth2.0_token(oauth_endpoints("twitter"), myapp)
# 3. Get OAuth credentials
twitter_token <- oauth1.0_token(oauth_endpoints("twitter"), myapp)
myapp <- oauth_app("ContentMining", key = "an3hwBQXNU7xwZmz3ISTHHryI",
secret = " I5pMQT5Hlh7G3zHhIBmmTC2f0AKCrJKBem69eTygwpfRu9hNBj")
twitter_token <- oauth1.0_token(oauth_endpoints$twitter, myapp)
twitter_token <- oauth2.0_token(oauth_endpoints$twitter, myapp)
twitter_token <- oauth2.0_token(oauth_endpoints("twitter"), myapp)
# 3. Get OAuth credentials
twitter_token <- oauth1.0_token(oauth_endpoints("twitter"), myapp)
myapp <- oauth_app("ContentMining", key = "an3hwBQXNU7xwZmz3ISTHHryI",
secret = "I5pMQT5Hlh7G3zHhIBmmTC2f0AKCrJKBem69eTygwpfRu9hNBj")
twitter_token <- oauth1.0_token(oauth_endpoints("twitter"), myapp)
twitter_token <- oauth2.0_token(oauth_endpoints("twitter"), myapp)
twitter_token <- oauth2.0_token(oauth_endpoints("twitter"), myapp)
req <- GET("https://api.twitter.com/1.1/statuses/home_timeline.json",
config(token = twitter_token))
req <- GET("https://api.twitter.com/1.1/statuses/home_timeline.json",
config(token = twitter_token))
req <- GET("https://api.twitter.com/1.1/statuses/home_timeline.json",
config(token = twitter_token))
stop_for_status(req)
content(req)
?GET
install.packages("twitteR")
library(twitteR)
api_key <- "an3hwBQXNU7xwZmz3ISTHHryI"
api_secret <- "I5pMQT5Hlh7G3zHhIBmmTC2f0AKCrJKBem69eTygwpfRu9hNBj"
access_token <- "174593932-saf9FxbAiAW1KwSHoYrubL1yuT4Or6Q9OvIc8Ppt"
access_token_secret <- "AA0ft0RJLbZz5mColacJZ37imPDp7pt6nS55AJNg"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
?setup_twitter_oauth
?searchTwitter
mh370 <- searchTwitter("#PrayForMH370", since = "2014-03-08", until = "2014-03-20", n = 1000)
myapp <- oauth_app("ContentMining", key = "an3hwBQXNU7xwZmz3ISTHHryI",
secret = "I5pMQT5Hlh7G3zHhIBmmTC2f0AKCrJKBem69eTygwpfRu9hNBj")
# 3. Get OAuth credentials
twitter_token <- oauth2.0_token(oauth_endpoints("twitter"), myapp)
mh370 <- searchTwitter("#PrayForMH370", since = "2014-03-08", until = "2014-03-20", n = 1000)
？tm
?tm
library("tm", lib.loc="C:/Users/v-shuolv/Documents/R/win-library/3.1")
library(Rwordseg)
teststring1 <- "或是当事人在第一时间出来辟谣，已远远不能满足如今网络信息传播的速度，而虚假的信息往往给政府、企业、及知名人士带来负面的影响，如金庸去世，到近期所谓铁观音迷魂抢劫引发网民恐慌."
segmentCN(teststring1)
library(tm)
library(Rwordseg)
library(RTextTools)
library(FSelector)
Infor.sweet <- read.csv("Data/sweetsong.csv", header = TRUE)
Infor.sad <- read.csv("Data/sadsong.csv", header = TRUE)
View(Infor.sweet)
removeEnglish <- function(x) {
gsub("[a-z]+|[A-Z]+", "", x)
}
a <- removeEnglish("asfasfasdf我是汉语")
a
b <- removeEnglish("asfasfasdf我是a汉语")
b
?segmentCn
?segmentCN
s <- Infor.sweet$lyric
s
word_s <- lapply(Infor.sweet$lyric,removeEnglish)
class(word_s)
head(word_s)
Infor.sweet <- read.csv("Data/sweetsong.csv",encoding="UTF-8",  header = TRUE)
Infor.sad <- read.csv("Data/sadsong.csv",encoding="UTF-8",  header = TRUE)
word_s <- lapply(Infor.sweet$lyric,removeEnglish)
head(word_s)
word_s <- lapply(word_s,segmentCN)
head(word_s)
# 形成corpus
song.sad.corpus <- corpus(song.sad)
song.sweet.corpus <- corpus(song.sweet)
# 合成甜蜜和伤感的语料库
song.corpus <- c(song.sad.corpus, song.sweet.corpus)
?Corpus
makeCorpus <- function(str1, str2) {
# 伤感歌曲分词 组成语料库
word.sad <- lapply(str1, removeEnglish)
word.sad <- lapply(word.sad, segmentCN)
corpus.sad <- Corpus(VectorSource(word.sad))
# 甜蜜歌曲分词 组成语料库
word.sweet <- lapply(str2, removeEnglish)
word.sweet <- lapply(word.sweet, segmentCN)
corpus.sweet <- Corpus(VectorSource(word.sweet))
# 合成预料库
corpus <- c(corpus.sad, corpus.sweet)
return(corpus)
}
# 形成corpus
song.sad.corpus <- corpus(song.sad)
song.sweet.corpus <- corpus(song.sweet)
# 合成甜蜜和伤感的语料库
song.corpus <- c(song.sad.corpus, song.sweet.corpus)
corpus <- makeCorpus(Infor.sweet$lyric, Infor.sad$lyric)
str(corpus)
dtm <- function(corpus, tfidf = FALSE) {
## 读取停止词
mystopwords <- readLines("material/stopwords.txt")
if (tfidf == TRUE) {
## 文档-词矩阵 词的长度大于1就纳入矩阵
cor.dtm <- DocumentTermMatrix(corpus, control = list(wordLengths = c(2,
Inf), stopwords = mystopwords, weighting = weightTfIdf))
} else {
cor.dtm <- DocumentTermMatrix(corpus, control = list(wordLengths = c(2,
Inf), stopwords = mystopwords))
}
## 去掉稀疏矩阵中低频率的词
cor.dtm <- removeSparseTerms(cor.dtm, 0.98)
## 使得每一行至少有一个词不为0 rowTotals <- apply(cor.dtm, 1, sum) cor.dtm <-
## cor.dtm[rowTotals > 0]
return(cor.dtm)
}
dtm <- function(corpus, tfidf = FALSE) {
## 读取停止词
mystopwords <- readLines("Data/stopwords.txt")
if (tfidf == TRUE) {
## 文档-词矩阵 词的长度大于1就纳入矩阵
cor.dtm <- DocumentTermMatrix(corpus, control = list(wordLengths = c(2,
Inf), stopwords = mystopwords, weighting = weightTfIdf))
} else {
cor.dtm <- DocumentTermMatrix(corpus, control = list(wordLengths = c(2,
Inf), stopwords = mystopwords))
}
## 去掉稀疏矩阵中低频率的词
cor.dtm <- removeSparseTerms(cor.dtm, 0.98)
## 使得每一行至少有一个词不为0 rowTotals <- apply(cor.dtm, 1, sum) cor.dtm <-
## cor.dtm[rowTotals > 0]
return(cor.dtm)
}
corpus.dtm <- dtm(corpus)
corpus.dtm.tfidf <- dtm(corpus, tfidf = TRUE)
song.sweet.wordcloud <- wordCloud(corpus.dtm.tfidf)
install.packages('wordcloud')
library(wordcloud)
song.sweet.wordcloud <- wordCloud(corpus.dtm.tfidf)
?wordcloud
song.sweet.wordcloud <- wordcloud(corpus.dtm.tfidf)
corpus.dtm.tfidf
corpus.dtm
inspect(corpus.dtm)
inspect(corpus.dtm.tfidf)
song.sweet.wordcloud <- wordcloud(corpus.dtm.tfidf)
m <- as.matrix(corpus.dtm.tfidf)
v <- sort(rowSums(m),decreasing=TRUE)
m
d <- data.frame(word = names(v),freq=v)
d
head(d)
wordcloud(d$word,d$freq)
